[Overfitting y Underfitting](https://www.youtube.com/watch?v=7-6X3DTt3R8)

[C√≥mo identificar el OVERFITTING en tu RED NEURONAL](https://www.youtube.com/watch?v=ZmLKqZYlYUI)


## Selecci√≥n de Caracter√≠sticas üéØ

La **Selecci√≥n de Caracter√≠sticas** (`Feature Selection`), un proceso crucial en el aprendizaje autom√°tico para mejorar el rendimiento de los modelos, reducir el sobreajuste (`overfitting`) y disminuir el tiempo de entrenamiento.

### **1. M√©todos de Filtrado (Filter Methods)** üìä

Estos m√©todos seleccionan caracter√≠sticas bas√°ndose en sus propiedades estad√≠sticas, **independientemente de cualquier algoritmo de machine learning**. Son r√°pidos y computacionalmente eficientes, pero su principal debilidad es que no consideran las interacciones entre las distintas caracter√≠sticas.

#### **a) Valor F de ANOVA (`ANOVA F-value`)**
* **Concepto**: Estima la relaci√≥n lineal entre cada caracter√≠stica de entrada (predictor) y la variable de salida. Un valor F alto indica una mayor linealidad y, por lo tanto, una mayor importancia de la caracter√≠stica.
* **Implementaci√≥n**: Se utiliza la funci√≥n `f_classif` de `scikit-learn` para calcular los valores F de cada caracter√≠stica del dataset Iris. Los resultados muestran que `petal length` y `petal width` son las caracter√≠sticas con la relaci√≥n lineal m√°s fuerte con la clase de la flor.

#### **b) Umbral de Varianza (`Variance Threshold`)**
* **Concepto**: Este m√©todo elimina las caracter√≠sticas que tienen una varianza por debajo de un umbral predefinido. La idea es que las caracter√≠sticas con poca o ninguna variaci√≥n aportan poco poder predictivo.
* **Implementaci√≥n**: Es fundamental **estandarizar los datos** antes de aplicar este filtro. Sin estandarizar, las varianzas pueden ser enga√±osas debido a las diferentes escalas. Usando `VarianceThreshold` de `scikit-learn` con un umbral, se elimina la caracter√≠stica `sepal width`, que es la que menos var√≠a en el conjunto de datos estandarizado.

#### **c) Informaci√≥n Mutua (`Mutual Information`)**
* **Concepto**: Mide la dependencia entre dos variables, capturando tanto relaciones lineales como no lineales. Cuantifica cu√°nta informaci√≥n sobre la variable de salida se obtiene al conocer una caracter√≠stica de entrada.
* **Implementaci√≥n**: Se emplea `mutual_info_classif`. Al igual que con ANOVA, los resultados indican que `petal length` y `petal width` son las caracter√≠sticas que comparten m√°s informaci√≥n con la variable objetivo, haci√©ndolas las m√°s relevantes.

---

### **2. M√©todos de Envoltura (Wrapper Methods)** üì¶

Estos m√©todos utilizan un **modelo de aprendizaje autom√°tico espec√≠fico** para evaluar subconjuntos de caracter√≠sticas. Buscan la combinaci√≥n de caracter√≠sticas que ofrece el mejor rendimiento para ese modelo en particular. Son m√°s potentes que los m√©todos de filtro porque detectan interacciones entre variables, pero son computacionalmente mucho m√°s costosos y tienen un mayor riesgo de sobreajuste.

#### **a) Selecci√≥n Secuencial de Caracter√≠sticas (`Sequential Feature Selection` - SFS)**
* **Concepto**: SFS es un m√©todo iterativo que agrega caracter√≠sticas una por una al conjunto de caracter√≠sticas seleccionadas. En cada paso, se a√±ade la caracter√≠stica que m√°s mejora el rendimiento del modelo.
* **Implementaci√≥n**: Se utiliza `SequentialFeatureSelector` de la librer√≠a `mlxtend` con un modelo de Regresi√≥n Log√≠stica. Se configura para que busque el mejor subconjunto de entre 1 y 4 caracter√≠sticas. El resultado final muestra que el mejor rendimiento se obtiene utilizando **todas las 4 caracter√≠sticas** del dataset Iris.

#### **b) Selecci√≥n Secuencial hacia Atr√°s (`Sequential Backward Selection` - SBS)**
* **Concepto**: Es el enfoque opuesto a SFS. Comienza con todas las caracter√≠sticas y, en cada iteraci√≥n, elimina la caracter√≠stica que menos afecta negativamente al rendimiento del modelo, hasta alcanzar el n√∫mero deseado de caracter√≠sticas.
* **Implementaci√≥n**: Al igual que con SFS, se usa `SequentialFeatureSelector` pero con el par√°metro `forward=False`. De nuevo, para el dataset Iris y el modelo de Regresi√≥n Log√≠stica, el mejor rendimiento se logra conservando **todas las caracter√≠sticas**.

---

### **3. M√©todos Integrados (Embedded Methods)** üß©

Estos m√©todos realizan la selecci√≥n de caracter√≠sticas **como parte del propio proceso de entrenamiento del modelo**. Son un punto intermedio entre los m√©todos de filtro y los de envoltura en t√©rminos de costo computacional y rendimiento.

#### **Selecci√≥n de Caracter√≠sticas usando Random Forest**
* **Concepto**: Los modelos basados en √°rboles, como Random Forest, calculan de forma natural la importancia de cada caracter√≠stica. Esta importancia se mide por cu√°nto contribuye cada caracter√≠stica a reducir la impureza (√≠ndice de Gini) en los nodos del √°rbol.
* **Implementaci√≥n**:
    1.  Se entrena un `RandomForestClassifier` con el conjunto de datos.
    2.  Se accede al atributo `feature_importances_` del modelo entrenado para obtener la puntuaci√≥n de importancia de cada caracter√≠stica. Los resultados confirman una vez m√°s que `petal length` y `petal width` son, con diferencia, las m√°s importantes.
    3.  Se utiliza `SelectFromModel`, una meta-transformador que selecciona caracter√≠sticas bas√°ndose en un umbral de importancia proporcionado por el estimador (en este caso, Random Forest).
    4.  Finalmente, se compara la precisi√≥n (`accuracy`) de un modelo Random Forest entrenado con todas las caracter√≠sticas frente a uno entrenado solo con las caracter√≠sticas seleccionadas por `SelectFromModel`. El notebook demuestra que **se puede lograr un rendimiento casi id√©ntico utilizando un subconjunto m√°s peque√±o de caracter√≠sticas**, lo que valida la eficacia del m√©todo.

---

# Transfer Learning

## ¬øQu√© es? üß†
El Aprendizaje por Transferencia es una t√©cnica de Machine Learning que consiste en reutilizar un modelo pre-entrenado en una tarea origen como punto de partida para una segunda tarea objetivo.

En Deep Learning, esto es especialmente poderoso porque las primeras capas de una red neuronal convolucional (CNN) tienden a aprender caracter√≠sticas muy generales y reutilizables (bordes, texturas, colores), mientras que las capas m√°s profundas aprenden caracter√≠sticas m√°s espec√≠ficas de la tarea original (ojos, ruedas de coche, etc.). Aprovechamos esas capas iniciales ya entrenadas, lo que nos da una ventaja considerable.


## Beneficios Principales üèÜ
* **Ahorro de Tiempo y Recursos:** Reduce dr√°sticamente el tiempo de entrenamiento y la necesidad de potentes GPUs.
* **Mejor Rendimiento con Pocos Datos:** Es la soluci√≥n ideal cuando no dispones de un dataset masivo para obtener alta precisi√≥n con un dataset peque√±o.
* **Acceso a Arquitecturas de Vanguardia:** Te permite utilizar arquitecturas de red extremadamente potentes y probadas (como ResNet, InceptionV3, EfficientNet), dise√±adas por equipos de investigaci√≥n de primer nivel, sin tener que implementarlas desde cero.

## T√©cnicas Comunes üõ†Ô∏è
1.  **Extracci√≥n de Caracter√≠sticas (Feature Extraction):**
    * **Qu√© hace:** Se "congela" el modelo base y solo se entrena un nuevo clasificador final.
    * **Cu√°ndo usarlo:** Ideal para datasets peque√±os. Es la aproximaci√≥n m√°s r√°pida y segura.

2.  **Ajuste Fino (Fine-Tuning):**
    * **Qu√© hace:** Se re-entrena una peque√±a parte de las capas finales del modelo base junto con el nuevo clasificador, usando una tasa de aprendizaje muy baja.
    * **Cu√°ndo usarlo:** Ideal para datasets m√°s grandes para "especializar" a√∫n m√°s el modelo a tus datos.


