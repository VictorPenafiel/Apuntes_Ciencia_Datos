# Machine learning
El machine learning es una rama de la inteligencia artificial (IA) centrada en entrenar a computadoras y m谩quinas para imitar el modo en que aprenden los humanos, realizar tareas de forma aut贸noma y mejorar su rendimiento y precisi贸n a trav茅s de la experiencia y la exposici贸n a m谩s datos.

[驴Qu茅 es el Machine Learning?驴Y Deep Learning? ](https://www.youtube.com/watch?v=KytW151dpqU&list=PL-Ogd76BhmcC_E2RjgIIJZd1DQdYHcVf0)

## Etiqueta (label o target) 
    Valor de salida o la variable objetivo que un modelo de aprendizaje supervisado est谩 tratando de predecir. 

## Regresi贸n lineal y m铆nimos cuadrados ordinarios
Es un algoritmo de aprendizaje supervisado que se utiliza para predecir el valor de una variable dependiente basada en el valor de una o m谩s variables independientes

y = ax = b

[Regresion_lineal](https://www.aprendemachinelearning.com/regresion-lineal-en-espanol-con-python/)

*Regresi贸n lineal y m铆nimos cuadrados ordinarios

[teor铆a](https://www.youtube.com/watch?v=k964_uNn3l0&ab_channel=DotCSV)

[proyecto](https://www.youtube.com/watch?v=w2RJ1D6kz-o&t=215s&ab_channel=DotCSV)


---

# Red neuronal
Las redes neuronales pretenden imitar aproximadamente la estructura del cerebro humano. Est谩n compuestas de muchos nodos interconectados (o neuronas), dispuestos en capas. Las redes neuronales hacen predicciones cuando los datos de entrada originales han realizado un "paso hacia adelante" a trav茅s de toda la red.

[Representacion_red_neuronal](https://playground.tensorflow.org/#activation=tanh&batchSize=1&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=30&networkShape=4,2&seed=0.76503&showTestData=false&discretize=false&percTrainData=10&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)

*Red neuronal

[Red_Neuronal_teor铆a](https://www.youtube.com/watch?v=MRIv2IwFTPg&t=453s&ab_channel=DotCSV)

[Red_Neuronal_teor铆a_2](https://www.youtube.com/watch?v=uwbHOpp9xkc&ab_channel=DotCSV)

[Backpropagation](https://www.youtube.com/watch?v=eNIqz_noix8&ab_channel=DotCSV)

[Matem谩ticas_Backpropagation](https://www.youtube.com/watch?v=M5QHwkkHgAA&ab_channel=DotCSV)

[Programando_Red_Neuronal](https://www.youtube.com/watch?v=W8AeOXa_FqU&ab_channel=DotCSV)

[Funciones de activaci贸n](https://developers.google.com/machine-learning/crash-course/neural-networks/activation-functions?hl=es-419)

---

## Funci贸n de activaci贸n
Las funciones de activaci贸n son reglas matem谩ticas que introducen la no-linealidad en las redes neuronales, permiti茅ndoles aprender patrones complejos. Sin ellas, una red profunda se comportar铆a como un simple modelo lineal.

En resumen, deciden qu茅 informaci贸n pasa de una neurona a la siguiente. 

### Rectified linear unit (ReLU)
Aplica una transformaci贸n no lineal muy simple, activa la neurona solo si el input est谩 por encima de cero. Mientras el valor de entrada est谩 por debajo de cero, el valor de salida es cero, pero cuando es superior de cero, el valor de salida aumenta de forma lineal con el de entrada.

### Sigmoide
La funci贸n sigmoide transforma valores en el rango de (-inf, +inf) a valores en el rango (0, 1).

### Tangente hiperb贸lica (Tanh)
La funci贸n de activaci贸n Tanh, se comporta de forma similar a la funci贸n sigmoide, pero su salida est谩 acotada en el rango (-1, 1).

[驴Qu茅 es la tasa de aprendizaje(learning rate) en el machine learning?](https://www.ibm.com/es-es/think/topics/learning-rate)

poca(epoch): Controla la duraci贸n total del entrenamiento.

Tama帽o del Lote(batch size): Determina la frecuencia de las actualizaciones de los pesos.

Tasa de Aprendizaje(learning rate): Define la magnitud del aprendizaje en cada actualizaci贸n.

Verbose: Se refiere al nivel de detalle que un algoritmo o un proceso de entrenamiento imprime en la consola.
    verbose=0: El algoritmo se ejecuta sin imprimir nada en la consola, a excepci贸n de posibles errores o advertencias cr铆ticas. Es 煤til cuando el c贸digo est谩 en producci贸n y no necesitas monitorear cada paso.

---
## Funci贸n de coste (loss function)
La funci贸n de coste (o funci贸n de p茅rdida) es una m茅trica que cuantifica el error de una red neuronal, midiendo la discrepancia entre el valor predicho y el valor real. Su objetivo es evaluar la precisi贸n del modelo en sus predicciones.

### Error cuadr谩tico medio 
El error cuadr谩tico se calcula como la diferencia al cuadrado entre el valor predicho y^ y el valor real y.

### Error medio absoluto
El error medio absoluto (mean absolute error, MAE) consiste en promediar el error absoluto de las predicciones.

### Log loss, logistic loss o cross-entropy loss 
En los problemas de clasificaci贸n, la capa de salida de una red neuronal emplea la funci贸n de activaci贸n Softmax. Esta funci贸n transforma las salidas del modelo en una distribuci贸n de probabilidad, donde cada valor representa la probabilidad de que la muestra de entrada pertenezca a una clase espec铆fica.

## backpropagation (retropropagaci贸n) 
Es una t茅cnica de machine learning esencial para la optimizaci贸n de las redes neuronales. Facilita el uso de algoritmos de descenso gradiente para actualizar las ponderaciones de la red. La l贸gica de la retropropagaci贸n es que las capas de neuronas de las redes neuronales artificiales son b谩sicamente una serie de funciones matem谩ticas anidadas. Durante el entrenamiento, esas ecuaciones interconectadas se anidan en otra funci贸n m谩s: una "funci贸n de p茅rdida" que mide la diferencia (o "p茅rdida") entre la salida deseada (o "verdad fundamental") para una entrada dada y la salida real de las redes neuronales.
Se actualizan los pesos y el bias.

[backpropagation](https://www.ibm.com/es-es/think/topics/backpropagation)

## Descenso del Gradiente
Algoritmo de optimizaci贸n que entrena  modelos de machine learning mediante la minimizaci贸n de errores entre los resultados previstos y los reales.

[gradient-descent](https://www.ibm.com/es-es/think/topics/gradient-descent)

[numerical-optimization](https://www.benfrederickson.com/numerical-optimization/)

*Descenso del Gradiente

[teor铆a](https://www.youtube.com/watch?v=A6FiCDoz8_4&t=322s&ab_channel=DotCSV)

[proyecto](https://www.youtube.com/watch?v=-_A_AAxqzCg&t=624s&ab_channel=DotCSV)

---

### Proceso de Entrenamiento de una Red Neuronal: Paso a Paso

El ciclo de aprendizaje de una red neuronal, conocido como **茅poca (epoch)**, se compone de los siguientes pasos fundamentales que se repiten iterativamente:

#### **1. Propagaci贸n hacia Adelante (Forward Propagation)** ★

El objetivo de esta fase es realizar una predicci贸n.

* Los **datos de entrada** (features) se introducen en la primera capa de la red.
* En cada neurona, las entradas se multiplican por sus **pesos (weights)** asociados, se suma un **sesgo (bias)** y el resultado pasa a trav茅s de una **funci贸n de activaci贸n** no lineal.
* La salida de una capa se convierte en la entrada de la siguiente, repitiendo el proceso hasta llegar a la capa de salida, que genera la **predicci贸n del modelo ($y_{pred}$)**.

#### **2. C谩lculo de la P茅rdida (Loss Calculation)** 

Aqu铆 medimos qu茅 tan equivocada estuvo la predicci贸n del modelo.

* Se compara la predicci贸n del modelo ($y_{pred}$) con el valor real y verdadero ($y_{real}$) utilizando una **funci贸n de p茅rdida (Loss Function)**, como el Error Cuadr谩tico Medio (MSE) o la Entrop铆a Cruzada.
* Esta funci贸n calcula un 煤nico valor num茅rico (el **error** o **p茅rdida**) que cuantifica la discrepancia total del modelo. Un error m谩s bajo significa una mejor predicci贸n.

#### **3. Propagaci贸n hacia Atr谩s y C谩lculo del Gradiente (Backpropagation)** 猬锔

Esta es la fase m谩s crucial, donde la red aprende de su error.

* Utilizando el algoritmo de **Backpropagation**, se calcula el **gradiente** de la funci贸n de p茅rdida. El gradiente es un vector que indica la direcci贸n en la que los pesos y biases deben ajustarse para reducir el error lo m谩s r谩pido posible.
* Este proceso comienza en la capa de salida y se propaga hacia atr谩s, capa por capa, distribuyendo la responsabilidad del error a cada par谩metro (peso y bias) de la red mediante la regla de la cadena del c谩lculo diferencial.

#### **4. Actualizaci贸n de Par谩metros (Parameter Update)** 锔

Finalmente, ajustamos los "tornillos" del modelo para que mejore en la siguiente iteraci贸n.

* Usando un algoritmo de optimizaci贸n (como el **Descenso de Gradiente**), se actualizan los **pesos y biases** en la direcci贸n opuesta al gradiente calculado.
* La magnitud de este ajuste est谩 controlada por un hiperpar谩metro clave: la **tasa de aprendizaje (learning rate)**. Una tasa de aprendizaje adecuada es crucial para una convergencia estable.

La f贸rmula simplificada para la actualizaci贸n de un peso ($W$) es:
`W_nuevo = W_viejo - tasa_de_aprendizaje * gradiente_del_error_respecto_a_W`

Este ciclo completo (Pasos 1-4) se repite para miles o millones de ejemplos de datos, permitiendo que el modelo minimice gradualmente su error y "aprenda" los patrones subyacentes en los datos.

---

# Aprendizaje supervisado
Regresi贸n- Variable numerica, predecir un valor numerico (Continua)

````
from sklearn.linear_model import LinearRegression

Datos: [[kil贸metros, a帽o]], precio
X = [[100000, 2010], [50000, 2015], [20000, 2020]]
y = [5000, 15000, 25000]  # Precios en USD

modelo = LinearRegression()
modelo.fit(X, y)
prediccion = modelo.predict([[80000, 2012]])
print(prediccion)  # Ejemplo: [12000.50]
````
-----------------------------------------------------------------------------------------------------

# Clasificaci贸n Etiquetas, predecir etiqueta o clase (Discretas)

    from sklearn.ensemble import RandomForestClassifier

# Datos: [[n煤m_palabras, contiene_emoji]], etiqueta

````
X = [[50, 0], [20, 1], [100, 0]]
y = [0, 1, 0]  # 0: No spam, 1: Spam

modelo = RandomForestClassifier()
modelo.fit(X, y)
prediccion = modelo.predict([[80, 1]])
print(prediccion)  # Ejemplo: [1] (spam)
````

------------------------------------------------------------------------------------------------------------
# Proyectos

[Ataque adversario](https://www.youtube.com/watch?v=JoQx39CoXW8&t=314s&ab_channel=DotCSV)

[Generando flores realistas con IA](https://www.youtube.com/watch?v=YsrMGcgfETY&t=4423s&ab_channel=DotCSV)

[Programa el juego de la vida](https://www.youtube.com/watch?v=qPtKv9fSHZY&ab_channel=DotCSV)


